{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvarelaj/nlp-miniproyecto-icesi/blob/main/Sesion%201/Actividad%20%237/analisis_sentimientos_quejas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ANÁLISIS DE SENTIMIENTO EN QUEJAS DE SOPORTE TÉCNICO (ARL)\n"
      ],
      "metadata": {
        "id": "P8j1BQx6WBVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-Proyecto: Análisis de Sentimientos con NLP\n",
        "## Corpus: Quejas y Reclamos ARL (Muestra Anonimizada)\n",
        "\n",
        "---\n",
        "\n",
        "### Contexto del proyecto\n",
        "Este proyecto aplica técnicas de **Procesamiento de Lenguaje Natural (NLP)** y **Sentiment Analysis** sobre un corpus real de 101 quejas recibidas por una Administradora de Riesgos Laborales (ARL) colombiana.\n",
        "\n",
        "En esta actividad, se busca replicar el flujo de trabajo del notebook de reseñas de películas, pero aplicado a un dominio específico y complejo: **quejas técnicas de una ARL en español**.\n",
        "\n",
        "El dataset ha sido **anonimizado previamente** para proteger la privacidad de los usuarios, sustituyendo nombres y documentos por la etiqueta `[ID_ANONIMIZADO]`. El objetivo principal es evaluar el desempeño de un modelo de lenguaje en la clasificación de polaridad (negativo, neutral, positivo) en un entorno de servicio al cliente.\n",
        "\n",
        "A diferencia del ejemplo base, aquí enfrentamos dos retos adicionales:\n",
        "* **Cambio de Idioma:** Se migra de VADER (inglés) a `pysentimiento`, un modelo basado en Deep Learning (BERT) optimizado para español.\n",
        "* **Dominio Técnico:** El lenguaje corporativo y técnico presenta matices de neutralidad y negatividad que difieren del lenguaje coloquial de las películas.\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Qué técnicas aplicamos en este Notebook?\n",
        "A lo largo de este notebook, realizaremos el ciclo completo de evaluación de un modelo de IA:\n",
        "\n",
        "| Paso | Técnica | Objetivo |\n",
        "|:---|:---|:---|\n",
        "| **1** | **Configuración** | Preparar el entorno con `pysentimiento` y `sklearn`. |\n",
        "| **2** | **Carga del Dataset** | Ingesta de la muestra anonimizada de 101 quejas. |\n",
        "| **3** | **Inferencia del Modelo** | Predicción automatizada de sentimientos usando IA. |\n",
        "| **4** | **Comparación (Ground Truth)** | Contrastar predicciones contra el etiquetado manual. |\n",
        "| **5** | **Métricas de Desempeño** | Calcular Accuracy, Precision y F1-Score. |\n",
        "| **6** | **Matriz de Confusión** | Visualizar dónde el modelo acierta y dónde se confunde. |\n",
        "\n",
        "---\n",
        "\n",
        "### Herramientas utilizadas\n",
        "- **pysentimiento**: Librería especializada en análisis de sentimientos para español.\n",
        "- **Scikit-learn**: Para el cálculo de métricas de desempeño (Accuracy, F1-Score).\n",
        "- **Pandas**: Para la estructuración y manipulación de la muestra.\n",
        "- **Seaborn & Matplotlib**: Para la generación del mapa de calor de la matriz de confusión."
      ],
      "metadata": {
        "id": "b4S1EPkLaQ5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 1 — Importación de librerías y configuración del entorno\n",
        "En esta etapa inicial preparamos el entorno técnico para el análisis de sentimientos. A diferencia del procesamiento clásico, aquí integramos:\n",
        "\n",
        "**pysentimiento:** Para utilizar modelos preentrenados (Transformers) especializados en la detección de polaridad en español.\n",
        "\n",
        "**pandas:** Para la manipulación del dataset de quejas y la estructuración de los resultados.\n",
        "\n",
        "**scikit-learn (sklearn):** Para el cálculo de métricas de calidad como la matriz de confusión y el reporte de accuracy.\n",
        "\n",
        "**matplotlib y seaborn:** Para generar las visualizaciones estadísticas que permiten interpretar el desempeño del modelo."
      ],
      "metadata": {
        "id": "Yuz5RWWfbZp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from spacy.matcher import Matcher\n",
        "from collections import Counter\n",
        "import spacy.cli\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "0Q1bid9ayu92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2 — Carga y exploración del dataset\n",
        "\n",
        "Procedemos a importar el archivo CSV que contiene las quejas directamente desde un repositorio público en GitHub usando pandas. Es fundamental que el dataset incluya la columna de texto original (descripcion_anonimizado) y, para este ejercicio de validación, la columna con el sentimiento real que (y_true) que fue etiquetada manualmente."
      ],
      "metadata": {
        "id": "NyjffXFKdoLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL de GitHub (raw content)\n",
        "url_datos = 'https://raw.githubusercontent.com/dvarelaj/nlp-miniproyecto-icesi/main/Sesion%201/Data%20Base/Quejas_Anonimizadas_Muestra_Etiqueta.csv'\n",
        "df = pd.read_csv(url_datos, delimiter=';')"
      ],
      "metadata": {
        "id": "hhGkN1SIyvZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 3 - Pre-Procesamiento\n",
        "Para garantizar la calidad de las predicciones, realizamos un flujo de limpieza.\n",
        "\n",
        "* **Carga de datos:** Se utiliza una muestra de **101 registros** anonimizados de la ARL.\n",
        "* **Limpieza:** Se eliminan valores nulos y celdas con espacios en blanco en la descripción de la queja para evitar errores en el tokenizador."
      ],
      "metadata": {
        "id": "9YRFfP30zlpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza básica\n",
        "df.dropna(subset=['descripcion_anonimizada'], inplace=True)\n",
        "df = df[df['descripcion_anonimizada'].str.strip() != '']\n",
        "\n",
        "print(f\"Total de quejas para analizar: {len(df)}\")"
      ],
      "metadata": {
        "id": "s37ZnuJezP-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 4 - Implementación del Modelo\n",
        "\n",
        "En este paso, recorremos cada una de las quejas y se las entregamos al modelo. El modelo analizará el contexto semántico de cada frase y nos devolverá una etiqueta: `POS` (Positivo), `NEU` (Neutral) o `NEG` (Negativo). Estos resultados se almacenarán en una nueva columna de nuestro DataFrame.\n",
        "\n",
        "Utilizaremos **Sentiment Analyzer** basado en Transformers, ya que nuestro archivo de datos tiene quejas y peticiones en español.\n",
        "\n",
        "Este modelo es superior para el español ya que:\n",
        "1. Entiende el contexto bidireccional de las frases.\n",
        "2. Detecta ironía y matices técnicos que modelos más simples ignorarían."
      ],
      "metadata": {
        "id": "wKN_dE66z_iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pysentimiento"
      ],
      "metadata": {
        "id": "8QTl5Stn0hM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysentimiento\n",
        "import pysentimiento\n",
        "from pysentimiento import create_analyzer\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Aplicamos el análisis a nuestras quejas\n",
        "df['sentiment_results'] = df['descripcion_anonimizada'].apply(lambda x: analyzer.predict(x))\n",
        "df['prediction'] = df['sentiment_results'].apply(lambda r: r.output)\n",
        "df['compound_score'] = df['sentiment_results'].apply(lambda r: r.probas['NEG'] * -1 if r.output == 'NEG' else r.probas['POS'])\n",
        "\n",
        "df[['descripcion_anonimizada', 'prediction']].head()"
      ],
      "metadata": {
        "id": "oNvT0OG90ww_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5 — Evaluación de resultados: Accuracy y Reporte de Clasificación\n",
        "\n",
        "Para determinar si nuestro modelo es confiable, comparamos sus predicciones contra nuestro \"estándar de oro\" (el etiquetado manual).\n",
        "* El **Accuracy** nos dirá el porcentaje total de aciertos.\n",
        "* El **Reporte de Clasificación** nos mostrará el desempeño detallado por cada categoría de sentimiento."
      ],
      "metadata": {
        "id": "3wGkzGUqf1pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conteo de los resultados obtenidos por el modelo\n",
        "conteo_sentimientos = df['prediction'].value_counts()\n",
        "print(conteo_sentimientos)"
      ],
      "metadata": {
        "id": "CQ1EMFIG1gxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones del modelo\n",
        "y_pred = [p.lower() for p in df['prediction'].tolist()]\n",
        "\n",
        "# Obtener las etiquetas verdaderas\n",
        "y_true = df['y_true'].tolist()\n",
        "\n",
        "# Generar el reporte (requires y_true)\n",
        "reporte = classification_report(y_true, y_pred)\n",
        "print(\"### Reporte de Clasificación ###\")\n",
        "print(reporte)\n",
        "\n",
        "# Calcular exactitud total (requires y_true)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy Total: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Yx5GBfs29PJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 6 — Visualización: Matriz de Confusión\n",
        "\n",
        "La matriz de confusión es la herramienta definitiva para entender el comportamiento de la IA. Nos permite ver, por ejemplo, cuántas quejas \"Negativas\" fueron clasificadas erróneamente como \"Neutrales\", ayudándonos a identificar sesgos o debilidades en el modelo."
      ],
      "metadata": {
        "id": "qGIBzIZugFDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la matriz de confusión\n",
        "labels = ['neg', 'neu', 'pos']\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# Graficar\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicción del Modelo (pysentimiento)')\n",
        "plt.ylabel('Etiqueta Real (Manual)')\n",
        "plt.title('Matriz de Confusión: Quejas ARL')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EWkothjZ-r-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ANÁLISIS DE RESULTADOS\n",
        "\n",
        "Una vez ejecutado el modelo sobre las 101 quejas, se presentan los siguientes hallazgos críticos basados en las métricas obtenidas:\n",
        "\n",
        "### A. RENDIMIENTO DEL MODELO\n",
        "El modelo supera significativamente la línea base del 50%, logrando un **Accuracy del 71%**. Sin embargo, el rendimiento es asimétrico:\n",
        "* **Alta Precisión en Negativos (0.86):** El modelo es muy efectivo detectando la molestia real del usuario en temas de fallas técnicas.\n",
        "* **Debilidad en Neutrales (Precision 0.37):** Existe una confusión importante entre quejas informativas y consultas de trámites.\n",
        "\n",
        "\n",
        "\n",
        "### B. OBSERVACIONES LINGÜÍSTICAS\n",
        "Como hallazgo, identificamos un fenómeno de **\"Ambigüedad de Dominio\"**:\n",
        "* Términos como *\"mantenimiento\"*, *\"portal no carga\"* o *\"no permite ingresar\"* son clasificados frecuentemente como **NEG** (negativos) por el modelo.\n",
        "* Lingüísticamente, estas frases podrían ser informativas (Neutrales), pero en el ecosistema de una ARL, el modelo interpreta correctamente la carga emocional de frustración implícita en la imposibilidad de realizar una tarea.\n",
        "\n",
        "### C. COMPARACIÓN CON EL MODELO BASE\n",
        "* **Desbalance Natural:** A diferencia del dataset de películas (balanceado 50/50), las quejas de la ARL están **naturalmente desbalanceadas hacia lo negativo**.\n",
        "* **Ausencia de Positivos:** La ausencia total de sentimientos positivos (**POS = 0**) refleja la realidad de los canales de soporte: el usuario rara vez se comunica para expresar satisfacción en un canal de reclamos."
      ],
      "metadata": {
        "id": "c8Hs18YT_45P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico de barras para visualizar el desbalance de sentimientos\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x='prediction', data=df, palette='viridis')\n",
        "plt.title('Distribución de Predicciones: Dominio ARL')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qztM9gWidSTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}